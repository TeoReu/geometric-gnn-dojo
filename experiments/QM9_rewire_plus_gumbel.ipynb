{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w6cXHeT-lA1h",
        "sYRa_BgE4a3C"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install modules"
      ],
      "metadata": {
        "id": "w6cXHeT-lA1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Sanity check torch version and GPU runtime\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"WARNING! You are running on a non-GPU instance. For this practical a GPU is highly recommended.\"\n",
        "REQUIRED_VERSION = \"1.13.1+cu116\"\n",
        "TORCH_VERSION = torch.__version__\n",
        "CUDA_VERSION = TORCH_VERSION.split(\"+\")\n",
        "\n",
        "if TORCH_VERSION != REQUIRED_VERSION:\n",
        "  print(f\"Detected torch version {TORCH_VERSION}, but notebook was created for {REQUIRED_VERSION}\")\n",
        "  print(f\"Attempting installation of {REQUIRED_VERSION}\")\n",
        "  !pip install torch==1.13.1+cu116\n",
        "print(\"Correct version of torch detected. You are running on a machine with GPU.\")\n"
      ],
      "metadata": {
        "id": "AHPweDxsFoGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af015c0-577c-4496-efc5-4e8e05f68678",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct version of torch detected. You are running on a machine with GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HwOueIhIQPBd",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c62abbd-e4b4-4ddb-efdc-e5ae2624acc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torch scatter\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling torch sparse\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling torch geometric\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.4/370.4 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.0/527.0 KB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing molecule relevant libraries.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Install required python libraries\n",
        "import os\n",
        "\n",
        "# Install PyTorch Geometric and other libraries\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print(\"Installing torch scatter\")\n",
        "    !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
        "    print(\"Installing torch sparse\")\n",
        "    !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
        "    print(\"Installing torch geometric\")\n",
        "    !pip install -q torch-geometric==2.0.3\n",
        "    print(\"Installing molecule relevant libraries.\")\n",
        "    !pip install -q rdkit-pypi==2021.9.4\n",
        "    !pip install -q py3Dmol==1.8.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mvIHO8B_RjeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a45a4a-c60f-4afb-b7a8-c7a08c38e233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "PyTorch version 1.13.1+cu116\n",
            "PyG version 2.0.3\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Import python modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats import ortho_group\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential, Sigmoid\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.datasets import QM9\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_scatter import scatter\n",
        "\n",
        "import rdkit.Chem as Chem\n",
        "from rdkit.Geometry.rdGeometry import Point3D\n",
        "from rdkit.Chem import QED, Crippen, rdMolDescriptors, rdmolops\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "import py3Dmol\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "voXIXbkVOeZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0cd857-d90e-4316-ecdf-267932e7c6dc",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All seeds set.\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rFNre1NLdMvT",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19540ae-8ebf-4e82-a8fb-46f23299ded6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions loaded.\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Helper functions for data preparation\n",
        "\n",
        "class SetTarget:\n",
        "    \"\"\"\n",
        "    This transform modifies the labels vector per data sample to only keep \n",
        "    the label for a specific target (there are 19 targets in QM9).\n",
        "\n",
        "    Note: for this practical, we have hardcoded the target to be target #0,\n",
        "    i.e. the electric dipole moment of a drug-like molecule.\n",
        "    (https://en.wikipedia.org/wiki/Electric_dipole_moment)\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        target = 0 # we hardcoded choice of target  \n",
        "        data.y = data.y[:, target]\n",
        "        return data\n",
        "\n",
        "\n",
        "class CompleteGraph:\n",
        "    \"\"\"\n",
        "    This transform adds all pairwise edges into the edge index per data sample, \n",
        "    then removes self loops, i.e. it builds a fully connected or complete graph\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "print(\"Helper functions loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExJ0b3xcQl5n",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23a83d7-1307-45f4-8bb3-a726e88407eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions added.\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Helper functions for visualization\n",
        "\n",
        "allowable_atoms = [\n",
        "    \"H\",\n",
        "    \"C\",\n",
        "    \"N\",\n",
        "    \"O\",\n",
        "    \"F\",\n",
        "    \"C\",\n",
        "    \"Cl\",\n",
        "    \"Br\",\n",
        "    \"I\",\n",
        "    \"H\", \n",
        "    \"Unknown\",\n",
        "]\n",
        "\n",
        "def to_atom(t):\n",
        "    try:\n",
        "        return allowable_atoms[int(t.argmax())]\n",
        "    except:\n",
        "        return \"C\"\n",
        "\n",
        "\n",
        "def to_bond_index(t):\n",
        "    t_s = t.squeeze()\n",
        "    return [1, 2, 3, 4][\n",
        "        int(\n",
        "            torch.dot(\n",
        "                t_s,\n",
        "                torch.tensor(\n",
        "                    range(t_s.size()[0]), dtype=torch.float, device=t.device\n",
        "                ),\n",
        "            ).item()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def to_rdkit(data, device=None):\n",
        "    has_pos = False\n",
        "    node_list = []\n",
        "    for i in range(data.x.size()[0]):\n",
        "        node_list.append(to_atom(data.x[i][:5]))\n",
        "\n",
        "    # create empty editable mol object\n",
        "    mol = Chem.RWMol()\n",
        "    # add atoms to mol and keep track of index\n",
        "    node_to_idx = {}\n",
        "    invalid_idx = set([])\n",
        "    for i in range(len(node_list)):\n",
        "        if node_list[i] == \"Stop\" or node_list[i] == \"H\":\n",
        "            invalid_idx.add(i)\n",
        "            continue\n",
        "        a = Chem.Atom(node_list[i])\n",
        "        molIdx = mol.AddAtom(a)\n",
        "        node_to_idx[i] = molIdx\n",
        "\n",
        "    added_bonds = set([])\n",
        "    for i in range(0, data.edge_index.size()[1]):\n",
        "        ix = data.edge_index[0][i].item()\n",
        "        iy = data.edge_index[1][i].item()\n",
        "        bond = to_bond_index(data.edge_attr[i])  # <font color='red'>TODO</font> fix this\n",
        "        # bond = 1\n",
        "        # add bonds between adjacent atoms\n",
        "\n",
        "        if data.edge_attr[i].sum() == 0:\n",
        "          continue\n",
        "\n",
        "        if (\n",
        "            (str((ix, iy)) in added_bonds)\n",
        "            or (str((iy, ix)) in added_bonds)\n",
        "            or (iy in invalid_idx or ix in invalid_idx)\n",
        "        ):\n",
        "            continue\n",
        "        # add relevant bond type (there are many more of these)\n",
        "\n",
        "        if bond == 0:\n",
        "            continue\n",
        "        elif bond == 1:\n",
        "            bond_type = Chem.rdchem.BondType.SINGLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 2:\n",
        "            bond_type = Chem.rdchem.BondType.DOUBLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 3:\n",
        "            bond_type = Chem.rdchem.BondType.TRIPLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 4:\n",
        "            bond_type = Chem.rdchem.BondType.SINGLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "\n",
        "        added_bonds.add(str((ix, iy)))\n",
        "\n",
        "    if has_pos:\n",
        "        conf = Chem.Conformer(mol.GetNumAtoms())\n",
        "        for i in range(data.pos.size(0)):\n",
        "            if i in invalid_idx:\n",
        "                continue\n",
        "            p = Point3D(\n",
        "                data.pos[i][0].item(),\n",
        "                data.pos[i][1].item(),\n",
        "                data.pos[i][2].item(),\n",
        "            )\n",
        "            conf.SetAtomPosition(node_to_idx[i], p)\n",
        "        conf.SetId(0)\n",
        "        mol.AddConformer(conf)\n",
        "\n",
        "    # Convert RWMol to Mol object\n",
        "    mol = mol.GetMol()\n",
        "    mol_frags = rdmolops.GetMolFrags(mol, asMols=True, sanitizeFrags=False)\n",
        "    largest_mol = max(mol_frags, default=mol, key=lambda m: m.GetNumAtoms())\n",
        "    return largest_mol\n",
        "\n",
        "\n",
        "def MolTo3DView(mol, size=(300, 300), style=\"stick\", surface=False, opacity=0.5):\n",
        "    \"\"\"Draw molecule in 3D\n",
        "    \n",
        "    Args:\n",
        "    ----\n",
        "        mol: rdMol, molecule to show\n",
        "        size: tuple(int, int), canvas size\n",
        "        style: str, type of drawing molecule\n",
        "               style can be 'line', 'stick', 'sphere', 'carton'\n",
        "        surface, bool, display SAS\n",
        "        opacity, float, opacity of surface, range 0.0-1.0\n",
        "    Return:\n",
        "    ----\n",
        "        viewer: py3Dmol.view, a class for constructing embedded 3Dmol.js views in ipython notebooks.\n",
        "    \"\"\"\n",
        "    assert style in ('line', 'stick', 'sphere', 'carton')\n",
        "\n",
        "    mol = Chem.AddHs(mol)\n",
        "    AllChem.EmbedMolecule(mol)\n",
        "    AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
        "    mblock = Chem.MolToMolBlock(mol)\n",
        "    viewer = py3Dmol.view(width=size[0], height=size[1])\n",
        "    viewer.addModel(mblock, 'mol')\n",
        "    viewer.setStyle({style:{}})\n",
        "    if surface:\n",
        "        viewer.addSurface(py3Dmol.SAS, {'opacity': opacity})\n",
        "    viewer.zoomTo()\n",
        "    return viewer\n",
        "\n",
        "def smi2conf(smiles):\n",
        "    '''Convert SMILES to rdkit.Mol with 3D coordinates'''\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        mol = Chem.AddHs(mol)\n",
        "        AllChem.EmbedMolecule(mol)\n",
        "        AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
        "        return mol\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(\"Helper functions added.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gY7foToFoo8Q"
      },
      "outputs": [],
      "source": [
        "# For storing experimental results over the course of the practical\n",
        "RESULTS = {}\n",
        "DF_RESULTS = pd.DataFrame(columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load QM9 dataset"
      ],
      "metadata": {
        "id": "xDDg-nTIhkkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load fully connected version"
      ],
      "metadata": {
        "id": "JKQ7TZFJtwzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ptep7GJJR8jJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348b6120-2f8e-43f8-fdd1-b756c4c7294c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/molnet_publish/qm9.zip\n",
            "Extracting qm9/raw/qm9.zip\n",
            "Downloading https://ndownloader.figshare.com/files/3195404\n",
            "Processing...\n",
            "100%|██████████| 133885/133885 [03:01<00:00, 736.94it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    path = './qm9'\n",
        "    target = 0\n",
        "\n",
        "    # Transforms which are applied during data loading:\n",
        "    # (1) Fully connect the graphs, (2) Select the target/label\n",
        "    transform = T.Compose([CompleteGraph(), SetTarget()])\n",
        "    \n",
        "    # Load the QM9 dataset with the transforms defined\n",
        "    dataset = QM9(path, transform=transform)\n",
        "\n",
        "    # Normalize targets per data sample to mean = 0 and std = 1.\n",
        "    mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
        "    std = dataset.data.y.std(dim=0, keepdim=True)\n",
        "    dataset.data.y = (dataset.data.y - mean) / std\n",
        "    mean, std = mean[:, target].item(), std[:, target].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load sparse/original version"
      ],
      "metadata": {
        "id": "9d4fyKlSBuCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load QM9 dataset with sparse graphs (by removing the full graphs transform)\n",
        "sparse_dataset = QM9(path, transform=SetTarget())\n",
        "\n",
        "# Normalize targets per data sample to mean = 0 and std = 1.\n",
        "mean = sparse_dataset.data.y.mean(dim=0, keepdim=True)\n",
        "std = sparse_dataset.data.y.std(dim=0, keepdim=True)\n",
        "sparse_dataset.data.y = (sparse_dataset.data.y - mean) / std\n",
        "mean, std = mean[:, target].item(), std[:, target].item()\n",
        "\n",
        "# Split datasets (3K subset)\n",
        "train_dataset_sparse = sparse_dataset[:1000]\n",
        "val_dataset_sparse = sparse_dataset[1000:2000]\n",
        "test_dataset_sparse = sparse_dataset[2000:3000]"
      ],
      "metadata": {
        "id": "ubaBa9m1shZf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preparation and Splitting"
      ],
      "metadata": {
        "id": "dIyUSCwjh4_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RFaBnrDoS2K-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee6e44b-8f8d-4468-b57b-e1ea04414393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 130831.\n",
            "Created dataset splits with 1000 training, 1000 validation, 1000 test samples.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of samples: {len(dataset)}.\")\n",
        "\n",
        "# Split datasets (in case of using the full dataset)\n",
        "# test_dataset = dataset[:10000]\n",
        "# val_dataset = dataset[10000:20000]\n",
        "# train_dataset = dataset[20000:]\n",
        "\n",
        "# Split datasets (our 3K subset)\n",
        "train_dataset = dataset[:1000]\n",
        "val_dataset = dataset[1000:2000]\n",
        "test_dataset = dataset[2000:3000]\n",
        "print(f\"Created dataset splits with {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples.\")\n",
        "\n",
        "# Create dataloaders with batch size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Atom features (`data.x`)** - $\\mathbb{R}^{|V| \\times 11}$\n",
        "- 1st-5th features: Atom type (one-hot: H, C, N, O, F)\n",
        "- 6th feature (also `data.z`): Atomic number (number of protons).\n",
        "- 7th feature: Aromatic (binary)\n",
        "- 8th-10th features: Electron orbital hybridization (one-hot: sp, sp2, sp3)\n",
        "- 11th feature: Number of hydrogens\n",
        "\n",
        "**Edge Index (`data.edge_index`)** - $\\mathbb{R}^{2×|E|}$\n",
        "- A tensor of dimensions 2 x `num_edges` that describe the edge connectivity of the graph\n",
        "\n",
        "**Edge features (`data.edge_attr`)** - $\\mathbb{R}^{|E|\\times 4}$\n",
        "- 1st-4th features: bond type (one-hot: single, double, triple, aromatic)\n",
        "\n",
        "**Atom positions (`data.pos`)** - $\\mathbb{R}^{|V|\\times 3}$\n",
        "- 3D coordinates of each atom . (We will talk about their importance later in the practical.)\n",
        "\n",
        "**Target (`data.y`)** - $\\mathbb{R}^{1}$\n",
        "- A scalar value corresponding to the molecules electric dipole moment\n",
        "\n",
        "We loaded the **fully-connected graphs** (i.e. all atoms in a molecule are connected to each other, except self-loops). The information about the molecule structures will be available to the models through the edge features (`data.edge_attr`) as follows:\n",
        "- When two atoms are physically connected, the edge attributes indicate the **bond type** (single, double, triple, or aromatic) through a one-hot vector.\n",
        "- When two atoms are not physically connected, **all edge attributes** are **zero**.\n",
        "We will later study the advantages/downsides of fully-connected adjacency matrices versus sparse adjacency matrices (where an edge between two atoms is present only when there exists a physical connection between them)."
      ],
      "metadata": {
        "id": "b8w8pZ4eeI3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = train_dataset[999] # one data sample, i.e. molecular graph\n",
        "print(\"Let us print all the attributes (along with their shapes) that our PyG molecular graph contains:\")\n",
        "print(data)\n",
        "Chem.Draw.MolsToGridImage([to_rdkit(data)])\n"
      ],
      "metadata": {
        "id": "wyEVg0zhMOTn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "106dced0-95f6-4d2b-f840-64ff472597d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let us print all the attributes (along with their shapes) that our PyG molecular graph contains:\n",
            "Data(x=[11, 11], edge_index=[2, 110], edge_attr=[110, 4], y=[1], pos=[11, 3], z=[11], name='gdb_1026', idx=[1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAADICAIAAAC7/QjhAAAABmJLR0QA/wD/AP+gvaeTAAAZaUlEQVR4nO3dfVRUdf4H8PcM8owCAz5SlmK2QegiohKrpZmhYrgpmnaIVj1U6o6Zu8vuntMPta2ttlYyjSBYJd0esI6GWia6PqGoq6EpaT6gJAgCwyAiMM4M9/fHtOSyMDPBzNwZv+/X8Q8P85m5n6Fzens/93u/VyFJEoiIiESllLsBIiIiOTEIiYhIaAxCIiISGoOQiIiExiAkIiKhMQiJiEhoDEIiIhIag5CIiITGICQiIqExCImISGgMQiIiEhqDkIiIhMYgJCIioTEIiYhIaAxCIiISGoOQiIiExiAkIiKhMQiJiEhoDEIiIhIag5CIiITGICQiIqExCImISGgMQiIiEhqDkIiIhMYgJCIioTEIiYhIaAxCIiISGoOQiIiExiAkIiKhMQiJiEhoDEIiIhIag5CIiITGICQiIqExCImISGgMQiIiEhqDkIiIhMYgJCIioTEIiYhIaAxCIiISGoOQiIiExiAkIiKhMQiJiEhoDEIiIhIag5CIiITGICQiIqExCImISGgMQiIiEhqDkIiIhMYgJCIioTEIiYhIaAxCIiISGoOQiIiExiAkIiKh9ZC7AZHU1+Pzz7FvH65dg5sb+vfHY49h+nR4ecndGRGRuBSSJMndgxg2bIBajfr69j8PCcH69Zg4UY6eiIiIo1HH+PhjJCejoQEvvICzZyFJkCT8+9948klUVCA+HoWFcrdIRCQonhHaX3U1QkPR2Ih16/Dss+1fXbIEq1fj3ntx7hzc3WVoj4hIbDwjtL9169DYiHHjOkhBAH/7G0JCcPkytm51dGNERMQgdISCAgB46qmOX/XwwMyZP5UREZFjMQjt7/RpABg2rNOCESN+KiMiIsfi7RP2p9UCQFBQpwXBwQCg0QDAJ59Aq0VQEIKDERSEoCCoVPDxcUijREQiYhDan1IJAGYWJZleMpW99RaOH++gxssLgYEIDMSAAejf/8e/t/sTEoKAAHt8AyKiOxiD0P4CA1FZidraTgtqagBApQKAp57CiBHQaKDRoK7ux7/odGhpQWUlKivx3Xedfo5CgaCgmlGjnqirU/1HUFCQqiO2/pJERK6KQWh/w4ahshInTmDs2I4LiosBYPhwAPjd7zquaW6GVovKSly9Cq22gz+VlaioQG1tU3394cOHzXekUCjMJ+VDDz3k7+/f9a9MROQ6eB+h/a1ahZdewujRKCqCQtH+1eZmDB6Mqips24apU7t1IKMRGk2dVnumtrauIxqNRqPR1NXVNTQ0mP+kjIyMxMTEIDPXNYmI7hQMQvvTajF4MOrrsWYNFi1q/6pajXffxQMP4PTpHy8T2p/BYOgwKU127dpVU1OzcePGp59+2jH9EBHJiEHoEJ9+ijlzAGDpUixejEGDAKCkBK++io8/hpcX9u3DqFHy9tjm73//+7Jly+bPn5+dnS13L0REdscgdJQNG7BkyY+3Uvj4oLUVLS0AcPfd2LABDz8sb3e3O378+MiRI4cMGXL+/Hm5eyEisjsGoQNdv47PP8fevaiuhlKJAQMwaRISEuDpKXdn/8VoNAYHB9fX15eXl4eEhMjdDhGRfTEIHaWsDPfcI3cT1po6deqXX3750UcfzTFNdImI7lzcYs0hqqpw330YPx4Gg9ytWOXhhx8GsG/fPrkbISKyOwahQ2RlQa9HYCB6uMaNmwxCIhIHR6P2ZzBg0CCUl+PrrzFpktzdWMVgMKhUqsbGxsrKyr59+8rdDhGRHfGM0P62b0d5OYYOxWOPyd2KtXr06BETEyNJ0v79++XuhYjIvhiE9peZCQALFnSwrYzc9Hp9Zy9xOkpEgmAQ2tmlS/j6a3h5Yd48uVv5L5cuXRo+fPjIkSM7K2AQEpEgGIR29sEHaG3FzJnmnkcoh7vuuuvixYunTp2qrq7usCA6OtrHx6ekpKTWzHMziIhcn3VBuHcvEhLg5weFAgoFfH0RH4+dO+3cm+vT6WDapez55+VupT13d3fTVcADBw50WODh4TFmzBgzBUREdwZLQWg0Qq3G+PHIz0dICBIT8cQT6NMH27fj8cexYAE6v8hE2LIFNTWIiEBsrNytdMDi8JPTUSISgaXb2l57De++i6Ag5OQgIeGnn3/1FZKSkJODoCC88YZdW3Rh778POOPpoAmDkIgIFu4jrKjA4MEwGHDwIMaMaf9qURHGjYMk4fvvERpq1y5dUkkJHnwQPXuiogI9e8rdTQd0Ol1gYGBLS0tNTU2Hjx7U6XQBAQG3bt2qra0NDAx0fIdERA5gdjS6fj1u3UJcXAcpCCAmBvHxMBqRk2On5lyb6ergnDnOmYIAPD09R48eLUlSYWFhZwWjRo1qbW3trICI6A5gNggPHgSAxx/vtGDKlJ/K6HZNTVi/HgAWLpS5E7M4HSUiMhuEly8DwJAhnRb84hcAcOmSLTu6M+Tlob4eo0dj+HC5WzGHQUhEZDYIGxoAmJvs9eoFANev27SlO4JzL5NpM2bMGE9Pz5MnT9bX13dYEBMT4+HhUVxc3FkBEZGrMxuEPj4A0NzcaYHpJV9fm7bk+r75BkeOQKXC7Nlyt2KBt7d3dHS00Wg82Ml828fHZ+TIkUaj8dChQw7ujYjIMcwGoemxA+XlnRb88AMA9Otn05ZcX1YWACQnw9tb7lYs43SUiARnNghHjQKAoqJOC0x7jowebdOWXFxDA/75TygUeO45uVuxCoOQiARn9j7CI0cwZgz8/XHhAoKD27+q0WDIENTXY88ePPLIf72k08HT0+a9uoaMDCxciAkTsHu33K1Y5ebNm4GBgZIk1dXV9ezoenBjY6NKpTJTQETk0syeEY4ejcmTcf06kpLaXynU6TBvHurrMX58+xSsrsZDD2HpUpv36hJaN24EgAUL5G7EWr6+vlFRUQaDobOrgH5+fpGRkQaDocjMbICIyGVZ2ms0Jwf33YcdOxAWhr/8Bdu2Yft2/PWvePBB5Odj0CDk5rZ/S0kJSkqQno7XX7dT007r6NGjA0+d2jxlCmbMkLuXn4HTUSISmaUg7N8fBw5g/nxcvYqXX8a0aYiPx5//jLIyJCfj4EHcfXf7t4wfj61b4emJP/1JtCxcu3ZtxY0bxyMj4eEhdy8/A4OQiERm9hrh7RobceAAKishSejXD2PH/ngTYWe2bEFiIgwGvPUWli2zSa9OTqPR3HXXXXq9vrS0dODAgXK38zM0NDSoVCqlUqnVan07uhnm+vXrQUFBbm5uWq3Wx3RTDRHRncLqB/P6+WHyZMybh/nzMXWqhRQEMH06Pv4Ybm74/e+RmdnNLl1Cbm5uS0tLXFyca6UggF69ekVGRur1+sOHD3dY4O/vP3z48Fu3bnVWQETkuuz5hPqZM5GTA4UCCxfin/+044GcgCRJGRkZAJ53+t1kOsTpKBEJy55BCCA5GatXo7UVycnIy7PvsWS1Z8+eCxcu3HvvvVNMG5G7GgYhEQmre0F48CD+9jcLNYsWYdUqGI1ISsK2bd06nBPLzMwEMH/+fKXSzv+2sI9x48a5ubkdOXKkuZMd9caOHatUKg8fPtzS0uLg3oiI7Kob/9eurcWUKfjDH/D22xYqX3wRaWm4dQuzZuFf/+r6EZ3V1atXN2/e7O7uvsB1bh9sx9/fPyIiQqfTHT16tMMClUplvoCIyEV1IwiDg5GTAzc3/O53ePNNC8XLl+OPf0RzM6ZNw/79XT+oU1q3bp1er09ISOjnytuucjpKRGLq3hxv5kxkZ0OpxB//iIwMC8V//SuWLUNTE6ZNw7//3a3jOhOj0ZiVlQWXXSbThkFIRGKy+j5CM9aswW9/C4UCWVkWthaTJDz/PLKyEBCA3bsxYkR3D+0Etm/fHh8fP3To0LNnzyoUCrnb6bq6urrevXt7enpqtVrPjraKra2t7dOnj7e3t1ar9XCpHQOIiMywxcqOxYuxatWPIffxx+YqFQpkZODpp1Ffj7g4fPedDY4uN9MymZSUFJdOQQAqlSosLKy5ufnYsWMdFgQHB4eFhTU1NXVWQETkimy0xPHFF/F//wejEcnJyM83e0AlcnMxaxZqajBpEkpLbdOATMrKyrZv3+7l5fXss8/K3YsNcDpKRAKy3Vr/FSuQmgq9HomJ+PJLc5Vubli/HhMmoKICcXENVVU268HhsrOzW1tbExMTg4KC5O7FBhiERCQgW1wjbCNJWLwY770Hb298+WX7xzO109xsSEiY39R06Nq1/fv39+/f32ZtOIperx84cGBVVVVhYWFsbKzc7dhAdXV1v379fH196+rq3N3d/7egqqpqwIABZgqIiFyOTe/+ViiwZg1SUtDcjPh4FBaaK/b2bvnss+90ugsXLkydOlWr1dqyE4f44osvqqqqIiIi7owUBNCnT5/777+/sbHxm2++6bCgX79+Q4cObWxsLC4udnBvRER2YuttUEzLYebOxc2biI/H8eNmav169dq9e3d0dHRxcfH48ePr6ups3IydmZbJWLxr4sSJEw5pxzY4HSUi0dhhPzClEh9+iMREXL+OuDiUlJip7dWr186dOyMjI0+ePDllypQbN27Yvh/7OHfu3O7du3v27JmUlGSm7IMPPoiMjHzuuef0er3DeusOBiERicY+G2O6uWHjRkyditpaTJiAs2fN1AYEBHz99ddhYWFHjhyZPHnyzZs37dKSrX3wwQeSJM2ZM6dnz55mytzc3Nzd3bOysmbNmuUSX+2RRx4BUFhYaDQaOywwBaGZAiIiFyPZT1OTNH68BEh33SWVlpqvLS8vHzx4MICJEyc2NzfbsatuKy0tzc7O7tWrF4CjR49arD906FDfvn0BhIeHl1r6PTiDIUOGADh27FhnBaGhoQCOHz/uyK6IiOzEno9K8PbGtm0YOxbl5XjsMVy9aqY2JCRkz54999xzz65du5566imnGiS2trYeP378jTfemDZtWt++fQcPHrxgwQLTmsns7GyLrcbExBQVFYWHh5eUlERHRzv/UJHTUSISi92jtrZWioi4Fhw8d8oUjUZjvvbcuXOm+yhmzJih1+vt3ptZJSUlGRkZc+bMGTBgwO2/sd69ez/55JNz5841ZeH06dMbGxstflpDQ0N8fDwAT0/P3NxcB/TfZXv27HnnnXfOnz/fWUFubi6AhIQER3ZFRGQn9g9CSZKqqqY/+iiA6Ojo69evm689depUcHAwgGeeecZoNDqivf/Q6XQFBQVpaWkTJ0708/O7PfzCwsJSUlLy8vIqKira6n/uzNNgMKjVatMHqtVqB387G7p8+TIAlUrlul+BiKiNQ4JQkq5du/bAAw8AGDVqVENDg/niEydOqFQqAPPmzWttbbVrYy0tLW3h5+vr25Z8CoUiKipKrVbn5eVdvXq1s7eXlpaGh4cDCAoK2rdvnzVHzMzMNJ1Kzpgx4+bNm7b7Kg51zz33ADh58qTcjRARdZeDglCSpCtXrpiWwzz00EMWZ4lFRUWm1ZhLliyxeSdNTU0FBQWpqamxsbE+Pj4dhl9VVZWVn9aFmefOnTsDAgIA/PKXv/zhhx+68VVk88wzzwBYvXq13I0QEXWX44JQkqSysjLTmYQ1S0MLCwtNp2gvv/xy9w9dX1+fn5+fmpoaFRXVo0ePtvBTKpVRUVGpqan5+fnV1dVd+/C2madCoUhNTbVmYHju3LmhQ4cCGDBggJn1mU4rJyfHdFIrdyNERN3l0CCUblsOk5CQcOvWLfPFO3fu9PLyAvDqq6924VharTYvL0+tVrcLP3d399jY2NTU1IKCAotzWuu1zTxnzpxpzcyztrbWtPzS19d38+bNtmrDAc6ePTtmzBhfX981a9bI3QsRUXc5OgglSfr2229Nz2qYMWOGwWAwX7xlyxZTurz55pvWfLhGo2kLPzc3t7bw8/DwmDhxYlpaWkFBwY0bN2zxPTpw+8zzypUrFut1Ol1ycrLpVDItLc1OXdmQXq9/++23TSuJVCpVcXGx3B0REXWXDEEoSVJxcXFgYCCA5ORki4PE9evXK5VKpVK5Y8eODgtqamry8vJSUlLCwsKUyp/ujPT09GwLP2vucLCJ77//3jTzDAkJsXLmmZ6ebmp73rx5Op3O3h12WUFBwX333WeaJ6vVaq1WK3dHREQ2IE8QSrcth1m8eLHF4oyMjNmzZ98eEj/88ENubm5KSoppAU4bb2/viRMnvv766wcOHJBrTWbbzNPPz2/Lli3WvOWzzz4zLduJjY3t8qVK+6murk5KSlIoFACioqJc8aImEVFnZAtCSZIOHDhgWg6zdOlSa+ovX76cmZmZlJTULvz8/f3j4+Nff/31Y8eOyX4bvklLS4tpXaX1M88TJ07cfffdAEJDQ7/77js7N2it1tbWzMxM0+m7n59fenq6k/yGiYhsRc4glCRp586dnp6eAF555ZUOCy5evJiZmZmYmNjuyb0BAQGJiYnp6enOE37/q23mOX/+fIsrgyRJqqioGDlyJIDAwMDdu3c7oEPzTp8+PXbsWNMvfMaMGeXl5XJ3RERkezIHoSRJmzdv7mw5zPnz528PPx8fn0cffXTlypX79u1z8o2527TNPH/1q19ZM/NsbGycPn06gB49esi4JrOxsVGtVpv+uwwcODA/P1+uToiI7E3+IJQkadOmTT169FAoFBkZGbf/vLW1NTQ0NC4u7rXXXjt48KA1J1VOqLi42DTzHDJkyJkzZyzWt7a2pqWlmbJfrVZbXFhrc1999ZVp+Ozm5paamuq6298QEVnDKYJQ+s/SUIVCkZ2dLXcvtldRUREVFWW65cDKmec//vEPDw8PAHFxcRY3aLWVK1eumHbJATB69GjeHUFEInCWIJQkac2aNaazkE8++UTuXmzv9pnn2rVrrXlLYWFh7969AURERFy+fNmu7RkMhvT0dH9/fwC9evXKzMzkhtpEJAgnCkJJklauXKlUKq3MCZfTbuZpTdJcuHDBtFl5cHDwgQMH7NTY0aNHR4wYYWosKSmpsrLSTgciInJCzhWEkiR98803crdgXzk5OaaZ5+TJk62ZeTY0NEyZMsW0P8CGDRts20xDQ4NarTbtP3f//fc7w1JVIiIHc7ogFEHbzHPYsGFlZWUW6/V6/eLFi9s29bbVo6ny8vJMzxz29PRMS0tramqyyccSEbkWhSRJIIe7ePFifHz82bNn+/fv/8UXX0RHR1t8S1ZW1qJFiwwGQ2JiYm5urre3d5ePXlZWtnjx4m3btgF49NFH33vvPdO2cEREAlJaLiE7CA0NPXTo0IQJEyorK8eNG/fRRx9ZfEtKSsonn3zi4+OzadOmOXPmdO24t27dWr58eVhY2LZt24KDg3NzcwsKCpiCRCQ0uU9JhabX6xcuXIj/7MRmzczz1KlTQ4cO7drCmUOHDg0bNsx0uJSUFI1G04UPISK6w3A0Kr933nln2bJlRqNx9uzZ69atszjz1Ol0pn3prKfRaJYuXbpx40ZJksLCwjIyMsaNG9eNlomI7hwMQqewY8eO2bNnNzQ0xMTEbNmypU+fPjb88A8//DA1NbWqqsrLy2v58uUvvfSSae80IiICg9B5nDp1atq0aWVlZYMGDdq6dWt4eHj3P/Ps2bMvvPDC3r17ATz++ONr164NDQ3t/scSEd1JuFjGWURERBQVFY0aNerSpUsxMTGmJZ1dptPpli9fHhkZuXfv3pCQkPz8/B07djAFiYj+F4PQifTv33/fvn1z5869cePG9OnTV69e3bXP2bVrV0RExIoVK/R6vVqtPn369LRp02zbKhHRHYOjUacjSdKKFStWrlwpSVJKSsqaNWusv6RXU1OzaNGiTZs2ARg5cmRmZmbb3mlERNQhBqGT+vTTT3/zm980NzdPmjTp008/DQgIMF/f2tr67rvvrlixQqvV9uzZ85VXXlm0aJFp7zQiIjKDQei8ioqKfv3rX1+7di08PHzr1q2DBg3qrLKkpOT5558vLCwEMHPmzPT09JCQEAd2SkTkwniN0HnFxMQUFRWFh4eXlJRER0fv37//f2saGxuXLFkSGRlZWFhoepT8pk2bmIJERNZjEDq1QYMGFRUVxcfHazSaSZMmffjhh7e/+tVXXw0fPnz16tWSJKWmpp45c4aLYoiIfi6ORl2A0Wh86aWXTItI1Wr1qlWrKioqFi5caLrFYsyYMe+///7w4cPlbpOIyCUxCF3GW2+9ZXoGU2xs7JkzZ+rq6nx9fZcvX/7iiy9yUQwRUZcxCF1JQUHBrFmzVCpVaWlpUlLSm2++2a9fP7mbIiJybQxCF/Ptt9/6+/ufPHnyiSeekLsXIqI7AYOQiIiExlWjREQkNAYhEREJjUFIRERCYxASEZHQGIRERCQ0BiEREQmNQUhEREJjEBIRkdAYhEREJDQGIRERCY1BSEREQmMQEhGR0BiEREQkNAYhEREJjUFIRERCYxASEZHQGIRERCQ0BiEREQmNQUhEREJjEBIRkdAYhEREJDQGIRERCY1BSEREQmMQEhGR0BiEREQkNAYhEREJjUFIRERCYxASEZHQGIRERCQ0BiEREQmNQUhEREJjEBIRkdAYhEREJDQGIRERCY1BSEREQmMQEhGR0BiEREQkNAYhEREJjUFIRERCYxASEZHQGIRERCQ0BiEREQmNQUhEREJjEBIRkdAYhEREJDQGIRERCY1BSEREQmMQEhGR0BiEREQkNAYhEREJjUFIRERCYxASEZHQ/h/OEDDtXrK1RAAAALJ6VFh0cmRraXRQS0wgcmRraXQgMjAyMS4wOS40AAB4nHu/b+09BiDgZUAAdihuYORg0AAJMLExZIBoZkY2iAALkgA6A6aHm4GRg4mRiYGJmYMJaAAzCwMLKwcTKxuDCCMrEzMLK5t4HFAZI9zSvSYmB2rcXtiBODElj/c7P1HYD5YQXrKP2aZrH4j9PCHYfkXWPLA4ayabQ8jyJ2DxF+/u2R91bbQHsad0VttZ2fWD2WIA/6YiAyqTOvIAAAEJelRYdE1PTCByZGtpdCAyMDIxLjA5LjQAAHicfVFRasMwDP33KXSBGEm2JfuzSUoZowls2e6w/92fSRupWxomOyC9PMtPzwE83ubXr2+4Bc8hAOA/u7UGnwkRwxU8gfF8eVlg2k7jjkzrx7K9g9pCX4/M07Zed4RghYFjpaLUAGMuSSRZgr/RjzJMMFBUlVYbDBiFa65HzORMjIkxl+oZS/Mzz8xsTIyVizT15pxUDH4mFiNyJEQS9I45SznsKEY0kZlExBI0szIf8NTGNmG5iP2mSNSw6gHvvMwPdv0ZOK7L3A1k/7pNDqTuhRWQ+8BkpfaxyKDSxbNB0jWSQXov5f5ir/c3tzz8AK+Cb2ELIdO/AAAAhXpUWHRTTUlMRVMgcmRraXQgMjAyMS4wOS40AAB4nBWMuw3DQAxDV0mZADpB0ukLw9UN4IU8fKSCzSMfn/sc7tzn4c/7XYLJFgKEatt9w7UYI7wKFqFLag4j3EJqOVC6zIKLMMW8AtqQHeMKMhH7rFTdZtVvyh7ASFSlo4maqzRhLir4vX8Y7x6K1f8l0AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from typing import Optional, Tuple\n",
        "import copy"
      ],
      "metadata": {
        "id": "6_l4eQ04q6vw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random dropout edges function"
      ],
      "metadata": {
        "id": "95WAkahOTOry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the drop out function for random drop out of connected carbon edges\n",
        "\n",
        "def dropout_edge(edge_index, p = 0.5, force_undirected = True, training = True):\n",
        "\n",
        "    if p < 0. or p > 1.:\n",
        "        raise ValueError(f'Dropout probability has to be between 0 and 1 '\n",
        "                         f'(got {p}')\n",
        "\n",
        "    if not training or p == 0.0:\n",
        "        edge_mask = edge_index.new_ones(edge_index.size(1), dtype=torch.bool)\n",
        "        return edge_index\n",
        "\n",
        "    row, col = edge_index\n",
        "\n",
        "    edge_mask = torch.rand(row.size(0), device=edge_index.device) >= p\n",
        "\n",
        "    if force_undirected:\n",
        "        edge_mask[row > col] = False\n",
        "\n",
        "    edge_index = edge_index[:, edge_mask]\n",
        "\n",
        "    if force_undirected:\n",
        "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
        "        edge_mask = edge_mask.nonzero().repeat((2, 1)).squeeze()\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "mS6k_CPtYE5f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carbon Backbone Rewiring: 3 schemes"
      ],
      "metadata": {
        "id": "tLSuzdI4B7dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Allow carbons to fully connect to any other nodes"
      ],
      "metadata": {
        "id": "nYOCXOjsxz-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define function that allow carbons to fully connect with all atoms and create new dataset\n",
        "def carbon_connect(dataset, sparse_dataset, threshold = 3, p = 0):\n",
        "\n",
        "    new_dataset = []\n",
        "    counter_dataset = []\n",
        "    idx = 0\n",
        "    for data in dataset:\n",
        "        data_copy = copy.copy(data)\n",
        "        sparse_data = sparse_dataset[idx]\n",
        "        # pick out all the virtual edges in the graph\n",
        "        virtual_edges_index = torch.index_select(data.edge_index, 1, torch.tensor([i for i in range(data.edge_index.shape[1]) if torch.sum(data.edge_attr[i]) == 0]))\n",
        "\n",
        "        # pick out all carbon nodes\n",
        "        carbon_nodes = []\n",
        "        for node in range(data.x.shape[0]):\n",
        "            if data.x[node,1] == 1:\n",
        "                carbon_nodes.append(node)\n",
        "\n",
        "        # if the number of carbons in a molecule is too small, no rewiring is performed\n",
        "        if len(carbon_nodes) <= 3:\n",
        "            new_dataset.append(data)\n",
        "            counter_dataset.append(data)\n",
        "            \n",
        "        else:\n",
        "\n",
        "        # crate carbon rewired graph that keeps only carbon related edges, add to new dataset\n",
        "            carbon_edge_index = torch.index_select(virtual_edges_index, 1, torch.tensor([i for i in range(virtual_edges_index.shape[1]) if virtual_edges_index[0,i] in carbon_nodes or virtual_edges_index[1,i] in carbon_nodes]))\n",
        "            carbon_edge_index = dropout_edge(carbon_edge_index, p)\n",
        "            carbon_edge_attr = torch.zeros(carbon_edge_index.shape[1],4)\n",
        "            data_copy.edge_index = torch.cat((sparse_data.edge_index, carbon_edge_index), dim = 1)\n",
        "            data_copy.edge_attr = torch.cat((sparse_data.edge_attr, carbon_edge_attr), dim = 0)\n",
        "            new_dataset.append(data_copy)\n",
        "\n",
        "\n",
        "            # create countering random connection graph dataset\n",
        "            data_copy_ = copy.copy(data)\n",
        "            extra_edge_num = carbon_edge_index.shape[1]//2\n",
        "            virtual_edges_list = [virtual_edges_index[:, i].unsqueeze(dim = 1) for i in range(virtual_edges_index.shape[1])]\n",
        "            sampled_edge_index = []\n",
        "            for j in range(extra_edge_num):\n",
        "                sample = random.choice(virtual_edges_list)\n",
        "\n",
        "                # check if repeated edges are sampled\n",
        "                sample_np = sample.numpy()\n",
        "                while np.any([np.array_equal(sample_np, tensor.numpy()) for tensor in sampled_edge_index]):\n",
        "                    sample = random.choice(virtual_edges_list)\n",
        "                    sample_np = sample.numpy()\n",
        "\n",
        "                # append both directions\n",
        "                sampled_edge_index.append(sample)\n",
        "                sampled_edge_index.append(torch.tensor([sample[1], sample[0]]).unsqueeze(dim = 1))\n",
        "\n",
        "            if len(sampled_edge_index) > 0:\n",
        "                sampled_edge_index = torch.cat(sampled_edge_index, dim = 1)\n",
        "                data_copy_.edge_index = torch.cat((sparse_data.edge_index, sampled_edge_index),dim = 1)\n",
        "                data_copy_.edge_attr = data_copy.edge_attr\n",
        "                counter_dataset.append(data_copy_)\n",
        "\n",
        "            else:\n",
        "                counter_dataset.append(sparse_data)\n",
        "\n",
        "        idx += 1\n",
        "        \n",
        "    return new_dataset, counter_dataset"
      ],
      "metadata": {
        "id": "b0HuXBF5n1Io"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Allow carbons only to connect with other carbons"
      ],
      "metadata": {
        "id": "K9K6RhB_x5Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define function that only allow carbons to fully connect and create new dataset\n",
        "def carbon_only_connect(dataset, sparse_dataset, threshold = 3, p = 0):\n",
        "\n",
        "    new_dataset = []\n",
        "    counter_dataset = []\n",
        "    idx = 0\n",
        "    for data in dataset:\n",
        "        data_copy = copy.copy(data)\n",
        "        sparse_data = sparse_dataset[idx]\n",
        "        # pick out all the virtual edges in the graph\n",
        "        virtual_edges_index = torch.index_select(data.edge_index, 1, torch.tensor([i for i in range(data.edge_index.shape[1]) if torch.sum(data.edge_attr[i]) == 0]))\n",
        "\n",
        "        # pick out all carbon nodes\n",
        "        carbon_nodes = []\n",
        "        for node in range(data.x.shape[0]):\n",
        "            if data.x[node,1] == 1:\n",
        "                carbon_nodes.append(node)\n",
        "\n",
        "        # if the number of carbons in a molecule is too small, no rewiring is performed\n",
        "        if len(carbon_nodes) <= 3:\n",
        "            new_dataset.append(data)\n",
        "            counter_dataset.append(data)\n",
        "            \n",
        "        else:\n",
        "\n",
        "        # crate carbon rewired graph that keeps only carbon-carbon edges, add to new dataset\n",
        "            carbon_edge_index = torch.index_select(virtual_edges_index, 1, torch.tensor([i for i in range(virtual_edges_index.shape[1]) if virtual_edges_index[0,i] in carbon_nodes and virtual_edges_index[1,i] in carbon_nodes]))\n",
        "            carbon_edge_index = dropout_edge(carbon_edge_index, p)\n",
        "            carbon_edge_attr = torch.zeros(carbon_edge_index.shape[1],4)\n",
        "            data_copy.edge_index = torch.cat((sparse_data.edge_index, carbon_edge_index), dim = 1)\n",
        "            data_copy.edge_attr = torch.cat((sparse_data.edge_attr, carbon_edge_attr), dim = 0)\n",
        "            new_dataset.append(data_copy)\n",
        "\n",
        "\n",
        "            # create countering random connection graph dataset\n",
        "            data_copy_ = copy.copy(data)\n",
        "            extra_edge_num = carbon_edge_index.shape[1]//2\n",
        "            virtual_edges_list = [virtual_edges_index[:, i].unsqueeze(dim = 1) for i in range(virtual_edges_index.shape[1])]\n",
        "            sampled_edge_index = []\n",
        "            for j in range(extra_edge_num):\n",
        "                sample = random.choice(virtual_edges_list)\n",
        "\n",
        "                # check if repeated edges are sampled\n",
        "                sample_np = sample.numpy()\n",
        "                while np.any([np.array_equal(sample_np, tensor.numpy()) for tensor in sampled_edge_index]):\n",
        "                    sample = random.choice(virtual_edges_list)\n",
        "                    sample_np = sample.numpy()\n",
        "\n",
        "                # append both directions\n",
        "                sampled_edge_index.append(sample)\n",
        "                sampled_edge_index.append(torch.tensor([sample[1], sample[0]]).unsqueeze(dim = 1))\n",
        "\n",
        "            if len(sampled_edge_index) > 0:\n",
        "                sampled_edge_index = torch.cat(sampled_edge_index, dim = 1)\n",
        "                data_copy_.edge_index = torch.cat((sparse_data.edge_index, sampled_edge_index),dim = 1)\n",
        "                data_copy_.edge_attr = data_copy.edge_attr\n",
        "\n",
        "                counter_dataset.append(data_copy_)\n",
        "\n",
        "            else:\n",
        "                counter_dataset.append(sparse_data)\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    return new_dataset, counter_dataset"
      ],
      "metadata": {
        "id": "q3M6csI_LBWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gambel trick random sampling of edges"
      ],
      "metadata": {
        "id": "t429nJT_Kx1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gumbel version, create corresponding gumbel dataset for a carbon_connected dataset\n",
        "def gumbel_connect(full_dataset, sparse_dataset, carbon_dataset):\n",
        "\n",
        "    gumbel_dataset = []\n",
        "\n",
        "    for i in range(len(full_dataset)):\n",
        "\n",
        "        full_data = copy.copy(full_dataset[i])\n",
        "        sparse_data = copy.copy(sparse_dataset[i])\n",
        "        carbon_data = copy.copy(carbon_dataset[i])\n",
        "\n",
        "        extra_edge_num = carbon_data.edge_index.shape[1] - sparse_data.edge_index.shape[1]\n",
        "        extra_edge_num = extra_edge_num // 2\n",
        "\n",
        "        if extra_edge_num == 0:\n",
        "            gumbel_dataset.append(sparse_data)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # pick out all the virtual edges in the graph\n",
        "            virtual_edges_index = torch.index_select(full_data.edge_index, 1, torch.tensor([j for j in range(full_data.edge_index.shape[1]) if torch.sum(full_data.edge_attr[j]) == 0]))\n",
        "\n",
        "            # create countering Gumbel random connection graph dataset\n",
        "\n",
        "            virtual_edges_list = [virtual_edges_index[:, j].unsqueeze(dim = 1) for j in range(virtual_edges_index.shape[1])]\n",
        "            scores = []\n",
        "            dis_cube_list = []\n",
        "            # do Gumbel-max trick\n",
        "            for edge in virtual_edges_list:\n",
        "                d_ij = torch.sum(torch.pow((full_data.pos[edge[0]] - full_data.pos[edge[1]]),2),-1).reshape(1)\n",
        "                d_ij_cube = torch.pow(d_ij,-3)\n",
        "                dis_cube_list.append(d_ij_cube)\n",
        "            dis_cube_list = torch.cat(dis_cube_list).tolist()\n",
        "\n",
        "            var = torch.var(torch.tensor(dis_cube_list))\n",
        "            for c in dis_cube_list:\n",
        "                z = -torch.log(-torch.log(torch.rand(1)))\n",
        "                score = c/var + z\n",
        "\n",
        "                scores.append(score)\n",
        "            # remove duplicated edges\n",
        "            sorted_dist_list, indices_sort = torch.sort(torch.tensor(dis_cube_list))\n",
        "            sorted_dist_list = sorted_dist_list.tolist()\n",
        "            indices_sort = indices_sort.tolist()\n",
        "            sorted_score_list = [scores[j] for j in indices_sort]\n",
        "\n",
        "            half_sorted_dist = sorted_dist_list[::2] \n",
        "            half_indices_sort = indices_sort[::2]\n",
        "            half_score_sorted = sorted_score_list[::2]\n",
        "\n",
        "            # select top k edges\n",
        "            selected = torch.topk(torch.tensor(half_score_sorted), extra_edge_num).indices\n",
        "            selected_indices = [half_indices_sort[j] for j in selected]\n",
        "            sampled_edges = [virtual_edges_list[i] for i in selected_indices]\n",
        "\n",
        "            # add both edge directions to sparse graph\n",
        "            sampled_edge_index = []\n",
        "            for sample in sampled_edges:\n",
        "                sampled_edge_index.append(sample)\n",
        "                sampled_edge_index.append(torch.tensor([sample[1], sample[0]]).unsqueeze(dim = 1))\n",
        "\n",
        "\n",
        "            sampled_edge_index = torch.cat(sampled_edge_index, dim = 1)\n",
        "            sparse_data.edge_index = torch.cat((sparse_data.edge_index, sampled_edge_index),dim = 1)\n",
        "            sparse_data.edge_attr = carbon_data.edge_attr\n",
        "            gumbel_dataset.append(sparse_data)\n",
        "\n",
        "\n",
        "        \n",
        "    return gumbel_dataset"
      ],
      "metadata": {
        "id": "jtUyCBPNeZF6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create rewired datasets for both schemes"
      ],
      "metadata": {
        "id": "SM-5VKhRsvNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_carbon, train_dataset_counter = carbon_connect(train_dataset, train_dataset_sparse)\n",
        "val_dataset_carbon, val_dataset_counter = carbon_connect(val_dataset, val_dataset_sparse)\n",
        "test_dataset_carbon, test_dataset_counter = carbon_connect(test_dataset, test_dataset_sparse)\n"
      ],
      "metadata": {
        "id": "s9J1HtpoAa7J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset_carbon_only, train_dataset_counter_only = carbon_only_connect(train_dataset, train_dataset_sparse)\n",
        "val_dataset_carbon_only, val_dataset_counter_only = carbon_only_connect(val_dataset, val_dataset_sparse)\n",
        "test_dataset_carbon_only, test_dataset_counter_only = carbon_only_connect(test_dataset, test_dataset_sparse)"
      ],
      "metadata": {
        "id": "h_RqbwH4LNXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_gumbel = gumbel_connect(train_dataset, train_dataset_sparse, train_dataset_carbon)\n",
        "val_dataset_gumbel = gumbel_connect(val_dataset, val_dataset_sparse, val_dataset_carbon)\n",
        "test_dataset_gumbel = gumbel_connect(test_dataset, test_dataset_sparse, test_dataset_carbon)"
      ],
      "metadata": {
        "id": "77H2Yp_-scLR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders with batch size = 32 carbon data\n",
        "train_loader_carbon = DataLoader(train_dataset_carbon, batch_size=32, shuffle=True)\n",
        "val_loader_carbon = DataLoader(val_dataset_carbon, batch_size=32, shuffle=False)\n",
        "test_loader_carbon = DataLoader(test_dataset_carbon, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_counter = DataLoader(train_dataset_counter, batch_size=32, shuffle=True)\n",
        "val_loader_counter = DataLoader(val_dataset_counter, batch_size=32, shuffle=False)\n",
        "test_loader_counter = DataLoader(test_dataset_counter, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "FFsgRcbNIUnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders with batch size = 32 carbon only data\n",
        "train_loader_carbon_only = DataLoader(train_dataset_carbon_only, batch_size=32, shuffle=True)\n",
        "val_loader_carbon_only = DataLoader(val_dataset_carbon_only, batch_size=32, shuffle=False)\n",
        "test_loader_carbon_only = DataLoader(test_dataset_carbon_only, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_counter_only = DataLoader(train_dataset_counter_only, batch_size=32, shuffle=True)\n",
        "val_loader_counter_only = DataLoader(val_dataset_counter_only, batch_size=32, shuffle=False)\n",
        "test_loader_counter_only = DataLoader(test_dataset_counter_only, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Mda03DrtQObK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders with batch size = 32 gumbel data\n",
        "train_loader_gumbel = DataLoader(train_dataset_gumbel, batch_size=32, shuffle=True)\n",
        "val_loader_gumbel = DataLoader(val_dataset_gumbel, batch_size=32, shuffle=False)\n",
        "test_loader_gumbel = DataLoader(test_dataset_gumbel, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "2Yp92ifRtBS1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GNN Model - Can be any benchmarking GNN"
      ],
      "metadata": {
        "id": "Q81BH5ZvFqEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wO5GskvZnZ1j"
      },
      "outputs": [],
      "source": [
        "class MPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"Message Passing Neural Network Layer\n",
        "\n",
        "        Args:\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
        "        \"\"\"\n",
        "        # Set the aggregation function\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # MLP `\\psi` for computing messages `m_ij`\n",
        "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
        "        # dims: (2d + d_e) -> d\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
        "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
        "        # dims: 2d -> d\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        The forward pass updates node features `h` via one round of message passing.\n",
        "\n",
        "        As our MPNNLayer class inherits from the PyG MessagePassing parent class,\n",
        "        we simply need to call the `propagate()` function which starts the \n",
        "        message passing procedure: `message()` -> `aggregate()` -> `update()`.\n",
        "        \n",
        "        The MessagePassing class handles most of the logic for the implementation.\n",
        "        To build custom GNNs, we only need to define our own `message()`, \n",
        "        `aggregate()`, and `update()` functions (defined subsequently).\n",
        "\n",
        "        Args:\n",
        "            h: (n, d) - initial node features\n",
        "            edge_index: (e, 2) - pairs of edges (i, j)\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "\n",
        "        Returns:\n",
        "            out: (n, d) - updated node features\n",
        "        \"\"\"\n",
        "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
        "        return out\n",
        "\n",
        "    def message(self, h_i, h_j, edge_attr):\n",
        "        \"\"\"Step (1) Message\n",
        "\n",
        "        The `message()` function constructs messages from source nodes j \n",
        "        to destination nodes i for each edge (i, j) in `edge_index`.\n",
        "\n",
        "        The arguments can be a bit tricky to understand: `message()` can take \n",
        "        any arguments that were initially passed to `propagate`. Additionally, \n",
        "        we can differentiate destination nodes and source nodes by appending \n",
        "        `_i` or `_j` to the variable name, e.g. for the node features `h`, we\n",
        "        can use `h_i` and `h_j`. \n",
        "        \n",
        "        This part is critical to understand as the `message()` function\n",
        "        constructs messages for each edge in the graph. The indexing of the\n",
        "        original node features `h` (or other node variables) is handled under\n",
        "        the hood by PyG.\n",
        "\n",
        "        Args:\n",
        "            h_i: (e, d) - destination node features, essentially h[edge_index[0]]\n",
        "            h_j: (e, d) - source node features, essentially h[edge_index[1]]\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "        \n",
        "        Returns:\n",
        "            msg: (e, d) - messages `m_ij` passed through MLP `\\psi`\n",
        "        \"\"\"\n",
        "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
        "        return self.mlp_msg(msg)\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        \"\"\"Step (2) Aggregate\n",
        "\n",
        "        The `aggregate` function aggregates the messages from neighboring nodes,\n",
        "        according to the chosen aggregation function ('sum' by default).\n",
        "\n",
        "        Args:\n",
        "            inputs: (e, d) - messages `m_ij` from destination to source nodes\n",
        "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
        "\n",
        "        Returns:\n",
        "            aggr_out: (n, d) - aggregated messages `m_i`\n",
        "        \"\"\"\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "    \n",
        "    def update(self, aggr_out, h):\n",
        "        \"\"\"\n",
        "        Step (3) Update\n",
        "\n",
        "        The `update()` function computes the final node features by combining the \n",
        "        aggregated messages with the initial node features.\n",
        "\n",
        "        `update()` takes the first argument `aggr_out`, the result of `aggregate()`, \n",
        "        as well as any optional arguments that were initially passed to \n",
        "        `propagate()`. E.g. in this case, we additionally pass `h`.\n",
        "\n",
        "        Args:\n",
        "            aggr_out: (n, d) - aggregated messages `m_i`\n",
        "            h: (n, d) - initial node features\n",
        "\n",
        "        Returns:\n",
        "            upd_out: (n, d) - updated node features passed through MLP `\\phi`\n",
        "        \"\"\"\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q0vqS1NAU_ZN"
      },
      "outputs": [],
      "source": [
        "class MPNNModel(Module):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
        "\n",
        "        Args:\n",
        "            num_layers: (int) - number of message passing layers `L`\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            in_dim: (int) - initial node feature dimension `d_n`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            out_dim: (int) - output dimension (fixed to 1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projection for initial node features\n",
        "        # dim: d_n -> d\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        \n",
        "        # Stack of MPNN layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        \n",
        "        # Global pooling/readout function `R` (mean pooling)\n",
        "        # PyG handles the underlying logic via `global_mean_pool()`\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Linear prediction head\n",
        "        # dim: d -> out_dim\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns: \n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            h = h + conv(h, data.edge_index, data.edge_attr) # (n, d) -> (n, d)\n",
        "            # Note that we add a residual connection after each MPNN layer\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
        "\n",
        "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
        "\n",
        "        return out.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oKraX_u9MXCS"
      },
      "outputs": [],
      "source": [
        "class EquivariantMPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"Message Passing Neural Network Layer\n",
        "\n",
        "        This layer is equivariant to 3D rotations and translations.\n",
        "\n",
        "        Args:\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
        "        \"\"\"\n",
        "        # Set the aggregation function\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        # Define the MLPs constituting your new layer.\n",
        "        # At the least, you will need `\\psi` and `\\phi` \n",
        "        # (but their definitions may be different from what\n",
        "        # we used previously).\n",
        "        #\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim + 1, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )  # MLP `\\psi`\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )  # MLP `\\phi`\n",
        "\n",
        "        # Define an additional MLP which maps the node message to a scalar, which\n",
        "        # acts as a coefficient in calculating the coordinate messages.\n",
        "        self.mlp_msg_x = Sequential(\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, 1), Sigmoid()\n",
        "          )  # MLP `\\psi_x`\n",
        "\n",
        "        # MLP `\\psi` for computing messages `m_ij`\n",
        "        # dims: d -> 1\n",
        "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
        "        # dims: 2d -> d\n",
        "        # MLP `\\psi_x` for computing coordinate message \n",
        "        # ===========================================\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        The forward pass updates node features `h` via one round of message passing.\n",
        "\n",
        "        Args:\n",
        "            h: (n, d) - initial node features\n",
        "            pos: (n, 3) - initial node coordinates\n",
        "            edge_index: (e, 2) - pairs of edges (i, j)\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "\n",
        "        Returns:\n",
        "            out: [(n, d),(n,3)] - updated node features\n",
        "        \"\"\"\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        # Notice that the `forward()` function has a new argument \n",
        "        # `pos` denoting the initial node coordinates. Your task is\n",
        "        # to update the `propagate()` function in order to pass `pos`\n",
        "        # to the `message()` function along with the other arguments.\n",
        "        #\n",
        "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr, pos = pos)\n",
        "        return out\n",
        "        # ==========================================\n",
        "\n",
        "    # ============ YOUR CODE HERE ==============\n",
        "    # Write custom `message()`, `aggregate()`, and `update()` functions\n",
        "    # which ensure that the layer is 3D rotation and translation equivariant.\n",
        "    #\n",
        "    # We are now updating both node features and coordinates in each layer, therefore \n",
        "    # there are two outputs for message, aggregate and update functions respectively.\n",
        "    def message(self, h_i, h_j, edge_attr, pos_i, pos_j):\n",
        "\n",
        "        # compute squared distance between destination and source nodes\n",
        "        dis_ij = torch.sum(torch.pow((pos_i - pos_j),2),-1).reshape(-1,1)\n",
        "\n",
        "        # compute the vector between destination and source nodes\n",
        "        dir_ij = pos_i - pos_j\n",
        "\n",
        "        msg = torch.cat([h_i, h_j, edge_attr, dis_ij], dim = 1)\n",
        "\n",
        "        # map the node feature message to a scalar psi_x\n",
        "        psi_x = self.mlp_msg_x(self.mlp_msg(msg))\n",
        "\n",
        "        # the coordinate message is chosen to be scalar projection of the node feature \n",
        "        # message times the directional vector between destination and source nodes\n",
        "        coor_msg = psi_x * dir_ij\n",
        "\n",
        "        # return both node feature messages and coordinate messages\n",
        "        return (self.mlp_msg(msg), coor_msg)\n",
        "    #\n",
        "    def aggregate(self, inputs, index):\n",
        "        # aggregation for node feature messages\n",
        "        agg_1 = scatter(inputs[0], index, dim=self.node_dim, reduce=self.aggr)\n",
        "\n",
        "        # aggregation for coordinate messages: mean\n",
        "        agg_2 = scatter(inputs[1], index, dim=self.node_dim, reduce=\"mean\")\n",
        "\n",
        "        return (agg_1, agg_2)\n",
        "    #\n",
        "    def update(self, aggr_out, h, pos):\n",
        "\n",
        "        # update of node features\n",
        "        upd_out_1 = torch.cat([h, aggr_out[0]], dim=-1)\n",
        "\n",
        "        # update of coordinates\n",
        "        upd_out_2 = aggr_out[1] + pos\n",
        "        \n",
        "        return (self.mlp_upd(upd_out_1),upd_out_2)\n",
        "    # ==========================================\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "\n",
        "class FinalMPNNModel(MPNNModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
        "\n",
        "        This model uses both node features and coordinates as inputs, and\n",
        "        is invariant to 3D rotations and translations (the constituent MPNN layers\n",
        "        are equivariant to 3D rotations and translations).\n",
        "\n",
        "        Args:\n",
        "            num_layers: (int) - number of message passing layers `L`\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            in_dim: (int) - initial node feature dimension `d_n`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            out_dim: (int) - output dimension (fixed to 1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projection for initial node features\n",
        "        # dim: d_n -> d\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        \n",
        "        # Stack of MPNN layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(EquivariantMPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        \n",
        "        # Global pooling/readout function `R` (mean pooling)\n",
        "        # PyG handles the underlying logic via `global_mean_pool()`\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Linear prediction head\n",
        "        # dim: d -> out_dim\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns: \n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
        "        pos = data.pos\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            # Message passing layer\n",
        "            h_update, pos_update = conv(h, pos, data.edge_index, data.edge_attr)\n",
        "            \n",
        "            # Update node features\n",
        "            h = h + h_update # (n, d) -> (n, d)\n",
        "            # Note that we add a residual connection after each MPNN layer\n",
        "            \n",
        "            # Update node coordinates\n",
        "            pos = pos_update # (n, 3) -> (n, 3)\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
        "\n",
        "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
        "\n",
        "        return out.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Experiments"
      ],
      "metadata": {
        "id": "SaIAP-DTIqfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FrYb8xr5iZQM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title [RUN] Helper functions for managing experiments, training, and evaluating models.\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data)\n",
        "        loss = F.mse_loss(y_pred, data.y)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def eval(model, loader, device):\n",
        "    model.eval()\n",
        "    error = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(data)\n",
        "            # Mean Absolute Error using std (computed when preparing data)\n",
        "            error += (y_pred * std - data.y * std).abs().sum().item()\n",
        "    return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "def run_experiment(model, model_name, train_loader, val_loader, test_loader, n_epochs=100):\n",
        "    \n",
        "    print(f\"Running experiment for {model_name}, training on {len(train_loader.dataset)} samples for {n_epochs} epochs.\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"\\nModel architecture:\")\n",
        "    print(model)\n",
        "    total_param = 0\n",
        "    for param in model.parameters():\n",
        "        total_param += np.prod(list(param.data.size()))\n",
        "    print(f'Total parameters: {total_param}')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Adam optimizer with LR 1e-3\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # LR scheduler which decays LR when validation metric doesn't improve\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.9, patience=5, min_lr=0.00001)\n",
        "    \n",
        "    print(\"\\nStart training:\")\n",
        "    best_val_error = None\n",
        "    perf_per_epoch = [] # Track Test/Val MAE vs. epoch (for plotting)\n",
        "    t = time.time()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # Call LR scheduler at start of each epoch\n",
        "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Train model for one epoch, return avg. training loss\n",
        "        loss = train(model, train_loader, optimizer, device)\n",
        "        \n",
        "        # Evaluate model on validation set\n",
        "        val_error = eval(model, val_loader, device)\n",
        "        \n",
        "        if best_val_error is None or val_error <= best_val_error:\n",
        "            # Evaluate model on test set if validation metric improves\n",
        "            test_error = eval(model, test_loader, device)\n",
        "            best_val_error = val_error\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            # Print and track stats every 10 epochs\n",
        "            print(f'Epoch: {epoch:03d}, LR: {lr:5f}, Loss: {loss:.7f}, '\n",
        "                  f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "        \n",
        "        scheduler.step(val_error)\n",
        "        perf_per_epoch.append((test_error, val_error, epoch, model_name))\n",
        "    \n",
        "    t = time.time() - t\n",
        "    train_time = t/60\n",
        "    print(f\"\\nDone! Training took {train_time:.2f} mins. Best validation MAE: {best_val_error:.7f}, corresponding test MAE: {test_error:.7f}.\")\n",
        "    \n",
        "    return best_val_error, test_error, train_time, perf_per_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully connected data"
      ],
      "metadata": {
        "id": "InQditTqFXMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7dc2c35-1119-4e14-be87-fe222ac72b7f",
        "id": "Op42CVpFVZS-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.2273386, Val MAE: 1.2132406, Test MAE: 0.6499045\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.1346457, Val MAE: 0.6747914, Test MAE: 0.5007994\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0796320, Val MAE: 0.6432015, Test MAE: 0.4492815\n",
            "Epoch: 040, LR: 0.000810, Loss: 0.0587108, Val MAE: 0.5408876, Test MAE: 0.4072644\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0446798, Val MAE: 0.4811877, Test MAE: 0.3746405\n",
            "Epoch: 060, LR: 0.000590, Loss: 0.0443795, Val MAE: 0.4668292, Test MAE: 0.3746405\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0172958, Val MAE: 0.4676628, Test MAE: 0.3746405\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0227568, Val MAE: 0.4688621, Test MAE: 0.3615971\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0175791, Val MAE: 0.4493251, Test MAE: 0.3615971\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0163181, Val MAE: 0.4257242, Test MAE: 0.3615971\n",
            "\n",
            "Done! Training took 2.95 mins. Best validation MAE: 0.4229135, corresponding test MAE: 0.3615971.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-67bcb2fcded8>:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader,\n",
        "    val_loader, \n",
        "    test_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carbon-fully connected data"
      ],
      "metadata": {
        "id": "mk5X6-qEFd_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIm8C-4wRVpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbfef93-f51a-4aae-b938-f47997b6adac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.3064021, Val MAE: 1.3116011, Test MAE: 0.8545199\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.1536000, Val MAE: 0.8429546, Test MAE: 0.5616040\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0804437, Val MAE: 0.5968016, Test MAE: 0.5211919\n",
            "Epoch: 040, LR: 0.000810, Loss: 0.0523507, Val MAE: 0.5845747, Test MAE: 0.4971798\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0628673, Val MAE: 0.5606824, Test MAE: 0.4621891\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0335820, Val MAE: 0.6025115, Test MAE: 0.4621891\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0429702, Val MAE: 0.5423394, Test MAE: 0.4538625\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0303794, Val MAE: 0.5840374, Test MAE: 0.4221595\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0251505, Val MAE: 0.5625510, Test MAE: 0.4221595\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0169075, Val MAE: 0.5291672, Test MAE: 0.4287997\n",
            "\n",
            "Done! Training took 2.04 mins. Best validation MAE: 0.5069623, corresponding test MAE: 0.4287997.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-e36654eb0675>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader_carbon,\n",
        "    val_loader_carbon, \n",
        "    test_loader_carbon,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Countering randomly rewired data"
      ],
      "metadata": {
        "id": "0WI36XFTFlYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader_counter,\n",
        "    val_loader_counter, \n",
        "    test_loader_counter,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UD6db6UEq2R",
        "outputId": "fbe22e1a-78a8-4f38-a58e-db62b85b5da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.3008476, Val MAE: 0.8660403, Test MAE: 0.6800021\n",
            "Epoch: 020, LR: 0.000900, Loss: 0.1464792, Val MAE: 1.1531581, Test MAE: 0.6325942\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.1423589, Val MAE: 0.9695161, Test MAE: 0.6173087\n",
            "Epoch: 040, LR: 0.000810, Loss: 0.0560814, Val MAE: 0.8060269, Test MAE: 0.6264480\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0490486, Val MAE: 0.7482422, Test MAE: 0.5969895\n",
            "Epoch: 060, LR: 0.000590, Loss: 0.0303307, Val MAE: 0.6814883, Test MAE: 0.5969895\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0269745, Val MAE: 0.7133568, Test MAE: 0.5969895\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0292075, Val MAE: 0.6796332, Test MAE: 0.5969895\n",
            "Epoch: 090, LR: 0.000349, Loss: 0.0408117, Val MAE: 0.7073276, Test MAE: 0.5969895\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0192947, Val MAE: 0.6795036, Test MAE: 0.5992665\n",
            "\n",
            "Done! Training took 1.93 mins. Best validation MAE: 0.6699507, corresponding test MAE: 0.5992665.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-b3a1f93db302>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Countering Gumbel Rewired Data"
      ],
      "metadata": {
        "id": "D6M3A1GWtnF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b47a2a8-caf4-4c2c-8d1b-19ffb44c1a5b",
        "id": "Ru0LPOOxtfoW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.2864470, Val MAE: 0.7882071, Test MAE: 0.6679169\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.1661555, Val MAE: 0.7148440, Test MAE: 0.5792535\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.1662946, Val MAE: 0.6513085, Test MAE: 0.5447224\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0642484, Val MAE: 0.6492974, Test MAE: 0.5193109\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0559486, Val MAE: 0.6681336, Test MAE: 0.5193109\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0540059, Val MAE: 0.6046419, Test MAE: 0.5142373\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0497252, Val MAE: 0.6392500, Test MAE: 0.5148587\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0314896, Val MAE: 0.6075156, Test MAE: 0.5148587\n",
            "Epoch: 090, LR: 0.000590, Loss: 0.0335793, Val MAE: 0.5802857, Test MAE: 0.4838277\n",
            "Epoch: 100, LR: 0.000478, Loss: 0.0258876, Val MAE: 0.5990437, Test MAE: 0.4838277\n",
            "\n",
            "Done! Training took 2.10 mins. Best validation MAE: 0.5600285, corresponding test MAE: 0.4838277.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-00eb89171f29>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader_gumbel,\n",
        "    val_loader_gumbel, \n",
        "    test_loader_gumbel,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carbon-only-connected data"
      ],
      "metadata": {
        "id": "pQ6SB2lDRM2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader_carbon_only,\n",
        "    val_loader_carbon_only, \n",
        "    test_loader_carbon_only,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yTxk3YfQ7CN",
        "outputId": "07f04868-7bab-4d79-f639-3d945387582f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.4170589, Val MAE: 0.9419421, Test MAE: 0.7274786\n",
            "Epoch: 020, LR: 0.000810, Loss: 0.2714769, Val MAE: 0.9665327, Test MAE: 0.6269974\n",
            "Epoch: 030, LR: 0.000810, Loss: 0.2524712, Val MAE: 0.8071654, Test MAE: 0.6387959\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.1310914, Val MAE: 0.7256642, Test MAE: 0.5990991\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.1818690, Val MAE: 0.7437262, Test MAE: 0.5788770\n",
            "Epoch: 060, LR: 0.000590, Loss: 0.1192776, Val MAE: 0.7265715, Test MAE: 0.5788770\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0690671, Val MAE: 0.6974664, Test MAE: 0.5885208\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0664813, Val MAE: 0.6880495, Test MAE: 0.5848289\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0485025, Val MAE: 0.6748483, Test MAE: 0.5625322\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0578050, Val MAE: 0.7103025, Test MAE: 0.5698421\n",
            "\n",
            "Done! Training took 2.15 mins. Best validation MAE: 0.6673919, corresponding test MAE: 0.5698421.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-9935e61132e0>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Countering randomly rewired data for carbon-only"
      ],
      "metadata": {
        "id": "2j4C7WEnRTwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    train_loader_counter_only,\n",
        "    val_loader_counter_only, \n",
        "    test_loader_counter_only,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IRzGueoQt-O",
        "outputId": "0d96a454-c7a1-4f42-befd-a2e3d9635dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.000900, Loss: 0.3575732, Val MAE: 0.9589049, Test MAE: 0.8701304\n",
            "Epoch: 020, LR: 0.000900, Loss: 0.4993797, Val MAE: 0.9287399, Test MAE: 0.7946330\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.2346774, Val MAE: 0.8447086, Test MAE: 0.6845413\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.1468037, Val MAE: 0.8038218, Test MAE: 0.6637792\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0977737, Val MAE: 0.8809986, Test MAE: 0.6637792\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0994950, Val MAE: 0.8502257, Test MAE: 0.6637792\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0561483, Val MAE: 0.8028808, Test MAE: 0.6845642\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0435592, Val MAE: 0.8063056, Test MAE: 0.6763274\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0478112, Val MAE: 0.8029435, Test MAE: 0.6763274\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0422369, Val MAE: 0.8153543, Test MAE: 0.6798657\n",
            "\n",
            "Done! Training took 2.20 mins. Best validation MAE: 0.7815316, corresponding test MAE: 0.6798657.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-fc2968cb995c>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carbon-only-connected data with dropout\n",
        "\n",
        "I have tried running for several different dropout rates, not explicitly shown here"
      ],
      "metadata": {
        "id": "plH4aVrNtWhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use d,c for dataset and counterdataset, and 1,2,3 for train, val and test, for convenience\n",
        "d1, c1 = carbon_only_connect(train_dataset, train_dataset_sparse,p=0.25) \n",
        "d2, c2 = carbon_only_connect(val_dataset, val_dataset_sparse,p=0.25)\n",
        "d3, c3 = carbon_only_connect(test_dataset, test_dataset_sparse,p=0.25)"
      ],
      "metadata": {
        "id": "pFXBGvIstgjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1_loader = DataLoader(d1, batch_size=32, shuffle=True)\n",
        "d2_loader = DataLoader(d2, batch_size=32, shuffle=True)\n",
        "d3_loader = DataLoader(d3, batch_size=32, shuffle=True)\n",
        "\n",
        "c1_loader = DataLoader(c1, batch_size=32, shuffle=True)\n",
        "c2_loader = DataLoader(c2, batch_size=32, shuffle=True)\n",
        "c3_loader = DataLoader(c3, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "XrOg2Rpzu3Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FinalMPNNModel(num_layers=4, emb_dim=59, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "\n",
        "model_name = type(model).__name__\n",
        "best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "    model, \n",
        "    model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "    d1_loader,\n",
        "    d2_loader, \n",
        "    d3_loader,\n",
        "    n_epochs=100\n",
        ")\n",
        "\n",
        "RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74ddb5d-a125-4260-b634-07bd22b83c69",
        "id": "cvI53UWSunPz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for FinalMPNNModel, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "FinalMPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=59, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (1): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (2): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "    (3): EquivariantMPNNLayer(emb_dim=59, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=59, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103196\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.4112321, Val MAE: 1.0495433, Test MAE: 0.7259091\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.2748447, Val MAE: 0.8709072, Test MAE: 0.6749871\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.2457653, Val MAE: 0.8200149, Test MAE: 0.6426071\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.1566708, Val MAE: 0.8197791, Test MAE: 0.6423136\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.1669516, Val MAE: 0.7746093, Test MAE: 0.6423136\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0955770, Val MAE: 0.7615547, Test MAE: 0.6423136\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0701834, Val MAE: 0.8189213, Test MAE: 0.6423136\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0503134, Val MAE: 0.7696099, Test MAE: 0.6413828\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0461621, Val MAE: 0.7508401, Test MAE: 0.6330348\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0483855, Val MAE: 0.7932731, Test MAE: 0.6229288\n",
            "\n",
            "Done! Training took 1.96 mins. Best validation MAE: 0.7228698, corresponding test MAE: 0.6229288.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-117-0aa0c03a6865>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "## label target: 0\n",
        "Fully connected: Test MAE 0.3616 for 2.95 *min* \n",
        "\n",
        "\\\\\n",
        "\n",
        "- Carbon connected: Test MAE 0.4287 for 2 *min*\n",
        "\n",
        "- Countering randomly rewired: Test MAE 0.5992 for 1.93 *min*\n",
        "\n",
        "- Countering Gumbel randomly rewired: Test MAE 0.4838 for 2.10 *min*\n",
        "\n",
        "\\\\\n",
        "\n",
        "- Carbon-only connected: Test MAE 0.5698 for 2.15 *min*\n",
        "\n",
        "- Carbon-only connected with dropout 0.25: Test MAE 0.6229 for 1.96 *min*\n",
        "\n",
        "- Countering randomly rewired for carbon-only: Test MAE 0.6798 for 2.2 *min*\n",
        "\n",
        "**carbon-connected dataset results in significantly more accurate prediction than the countering randomly rewired dataset. Dropout edges does not appear to help yet**"
      ],
      "metadata": {
        "id": "d6rRNhurLyBB"
      }
    }
  ]
}